{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c636e251",
   "metadata": {},
   "source": [
    "# Classical Regression Attempt to Fit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5071778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8413e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "# Prepare the \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 4-parameter ML dataset with Kbearing at the C-vertex\n",
    "four_params_Kbearing_c = np.load('data/major_params_Kbearing_c.npy')\n",
    "\n",
    "# Define feature and target columns\n",
    "feature_cols = [0, 1, 2, 3]  # height_factor, thickness, w_over_r, a_over_c\n",
    "target_cols = [4]  # Kbearing_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9353a",
   "metadata": {},
   "source": [
    "## Classical Regression Techniques for Multi-Parameter Modeling\n",
    "\n",
    "### Technical Background\n",
    "\n",
    "**1. Multiple Linear Regression (MLR)**\n",
    "- Form: $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\epsilon$\n",
    "- Assumes linear relationships between features and target\n",
    "- Uses ordinary least squares (OLS) to minimize residuals\n",
    "\n",
    "**2. Polynomial Regression**\n",
    "- Adds polynomial terms: $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_2 + \\beta_4 x_2^2 + ...$\n",
    "- Captures non-linear relationships\n",
    "- Can include interaction terms: $\\beta_{ij} x_i x_j$\n",
    "\n",
    "**3. Power Law Models**\n",
    "- Form: $y = A \\cdot x_1^{\\alpha_1} \\cdot x_2^{\\alpha_2} \\cdot x_3^{\\alpha_3} \\cdot x_4^{\\alpha_4}$\n",
    "- Common in engineering/physics applications\n",
    "- Linearized via log transformation: $\\log y = \\log A + \\alpha_1 \\log x_1 + \\alpha_2 \\log x_2 + ...$\n",
    "\n",
    "**4. Exponential Models**\n",
    "- Form: $y = A e^{\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4}$\n",
    "- Useful for growth/decay phenomena\n",
    "- Linearized: $\\log y = \\log A + \\beta_1 x_1 + \\beta_2 x_2 + ...$\n",
    "\n",
    "**5. Mixed Models with Interaction Terms**\n",
    "- Combines linear, polynomial, and interaction effects\n",
    "- Example: $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\beta_4 x_1^2 + ...$\n",
    "\n",
    "### Implementation Strategy\n",
    "1. Start with simple linear regression\n",
    "2. Add polynomial terms systematically  \n",
    "3. Test interaction terms between parameters\n",
    "4. Use statistical tests (R², F-test, p-values) to evaluate model quality\n",
    "5. Apply regularization (Ridge/Lasso) if needed for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d935d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and targets\n",
    "X = four_params_Kbearing_c[:, feature_cols]  # [height_factor, thickness, w_over_r, a_over_c]\n",
    "y = four_params_Kbearing_c[:, target_cols].ravel()  # Kbearing_c\n",
    "\n",
    "# Create feature names for interpretability\n",
    "feature_names = ['height_factor', 'thickness', 'w_over_r', 'a_over_c']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd902fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Multiple Linear Regression (Baseline)\n",
    "print(\"=== 1. Multiple Linear Regression ===\")\n",
    "mlr = LinearRegression()\n",
    "mlr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_mlr = mlr.predict(X_test)\n",
    "r2_mlr = r2_score(y_test, y_pred_mlr)\n",
    "mse_mlr = mean_squared_error(y_test, y_pred_mlr)\n",
    "\n",
    "print(f\"R² Score: {r2_mlr:.4f}\")\n",
    "print(f\"MSE: {mse_mlr:.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse_mlr):.4f}\")\n",
    "\n",
    "# Display the linear equation\n",
    "print(\"\\nLinear Model Equation:\")\n",
    "print(f\"Kbearing_c = {mlr.intercept_:.4f}\")\n",
    "for i, (name, coef) in enumerate(zip(feature_names, mlr.coef_)):\n",
    "    print(f\"           + {coef:.4f} * {name}\")\n",
    "\n",
    "# Statistical significance testing\n",
    "def calculate_p_values(model, X, y):\n",
    "    \"\"\"Calculate p-values for regression coefficients\"\"\"\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    \n",
    "    # Predictions and residuals\n",
    "    y_pred = model.predict(X)\n",
    "    residuals = y - y_pred\n",
    "    mse = np.sum(residuals**2) / (n - k - 1)\n",
    "    \n",
    "    # Covariance matrix\n",
    "    X_with_intercept = np.column_stack([np.ones(n), X])\n",
    "    cov_matrix = mse * np.linalg.inv(X_with_intercept.T @ X_with_intercept)\n",
    "    \n",
    "    # Standard errors and t-statistics\n",
    "    std_errors = np.sqrt(np.diag(cov_matrix))\n",
    "    coefficients = np.concatenate([[model.intercept_], model.coef_])\n",
    "    t_stats = coefficients / std_errors\n",
    "    \n",
    "    # P-values (two-tailed test)\n",
    "    p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), n - k - 1))\n",
    "    \n",
    "    return p_values, std_errors\n",
    "\n",
    "p_values, std_errors = calculate_p_values(mlr, X_train, y_train)\n",
    "print(\"\\nStatistical Significance:\")\n",
    "print(f\"Intercept: p-value = {p_values[0]:.4f}\")\n",
    "for i, (name, p_val) in enumerate(zip(feature_names, p_values[1:])):\n",
    "    significance = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "    print(f\"{name}: p-value = {p_val:.4f} {significance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Polynomial Regression with Interaction Terms\n",
    "print(\"\\n=== 2. Polynomial Regression with Interactions ===\")\n",
    "\n",
    "# Create polynomial features (degree=2 includes interactions)\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# Fit polynomial regression\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_poly = poly_reg.predict(X_test_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "\n",
    "print(f\"R² Score: {r2_poly:.4f}\")\n",
    "print(f\"MSE: {mse_poly:.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse_poly):.4f}\")\n",
    "\n",
    "# Display feature names and coefficients\n",
    "feature_names_poly = poly_features.get_feature_names_out(feature_names)\n",
    "print(f\"\\nPolynomial Model ({len(feature_names_poly)} terms):\")\n",
    "print(f\"Kbearing_c = {poly_reg.intercept_:.4f}\")\n",
    "for name, coef in zip(feature_names_poly, poly_reg.coef_):\n",
    "    if abs(coef) > 1e-6:  # Only show significant coefficients\n",
    "        print(f\"           + {coef:.6f} * {name}\")\n",
    "\n",
    "print(f\"\\nImprovement over linear: {r2_poly - r2_mlr:.4f} R² points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Extended Polynomial Regression with Higher Orders and Inverse Terms\n",
    "print(\"\\n=== 2b. Extended Polynomial with Higher Orders and Inverse Terms ===\")\n",
    "\n",
    "# Create extended feature matrix with higher order and inverse terms\n",
    "def create_extended_polynomial_features(X, max_degree=3, min_degree=-3):\n",
    "    \"\"\"\n",
    "    Create extended polynomial features including inverse terms\n",
    "    X: input features\n",
    "    max_degree: maximum polynomial degree (positive)\n",
    "    min_degree: minimum polynomial degree (negative for inverse terms)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    extended_features = []\n",
    "    feature_names_extended = []\n",
    "    \n",
    "    # Add original features (degree 1)\n",
    "    extended_features.append(X)\n",
    "    feature_names_extended.extend([f\"{name}\" for name in feature_names])\n",
    "    \n",
    "    # Add positive polynomial terms (degree 2 to max_degree)\n",
    "    for degree in range(2, max_degree + 1):\n",
    "        for i in range(n_features):\n",
    "            extended_features.append((X[:, i] ** degree).reshape(-1, 1))\n",
    "            feature_names_extended.append(f\"{feature_names[i]}^{degree}\")\n",
    "    \n",
    "    # Add inverse terms (degree -1 to min_degree)\n",
    "    for degree in range(-1, min_degree - 1, -1):\n",
    "        for i in range(n_features):\n",
    "            # Avoid division by zero - add small epsilon if needed\n",
    "            safe_feature = X[:, i] + 1e-10 * (X[:, i] == 0)\n",
    "            extended_features.append((safe_feature ** degree).reshape(-1, 1))\n",
    "            feature_names_extended.append(f\"{feature_names[i]}^{degree}\")\n",
    "    \n",
    "    # Add interaction terms (x_i * x_j)\n",
    "    for i in range(n_features):\n",
    "        for j in range(i + 1, n_features):\n",
    "            interaction = (X[:, i] * X[:, j]).reshape(-1, 1)\n",
    "            extended_features.append(interaction)\n",
    "            feature_names_extended.append(f\"{feature_names[i]} * {feature_names[j]}\")\n",
    "    \n",
    "    # Add mixed polynomial-inverse interactions (x_i^2 * x_j^-1, etc.)\n",
    "    for i in range(n_features):\n",
    "        for j in range(n_features):\n",
    "            if i != j:\n",
    "                # x_i^2 * x_j^-1\n",
    "                safe_feature_j = X[:, j] + 1e-10 * (X[:, j] == 0)\n",
    "                mixed_term = (X[:, i]**2 * safe_feature_j**(-1)).reshape(-1, 1)\n",
    "                extended_features.append(mixed_term)\n",
    "                feature_names_extended.append(f\"{feature_names[i]}^2 * {feature_names[j]}^-1\")\n",
    "    \n",
    "    return np.hstack(extended_features), feature_names_extended\n",
    "\n",
    "# Create extended polynomial features for training and test sets\n",
    "X_train_extended, feature_names_extended = create_extended_polynomial_features(X_train, max_degree=3, min_degree=-3)\n",
    "X_test_extended, _ = create_extended_polynomial_features(X_test, max_degree=3, min_degree=-3)\n",
    "\n",
    "print(f\"Extended feature matrix shape: {X_train_extended.shape}\")\n",
    "print(f\"Number of features: {len(feature_names_extended)}\")\n",
    "\n",
    "# Check for infinite or NaN values\n",
    "if np.any(np.isinf(X_train_extended)) or np.any(np.isnan(X_train_extended)):\n",
    "    print(\"WARNING: Extended features contain infinite or NaN values!\")\n",
    "    # Replace inf and nan with large/small finite values\n",
    "    X_train_extended = np.nan_to_num(X_train_extended, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "    X_test_extended = np.nan_to_num(X_test_extended, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "\n",
    "# Fit extended polynomial regression\n",
    "extended_poly_reg = LinearRegression()\n",
    "extended_poly_reg.fit(X_train_extended, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_extended = extended_poly_reg.predict(X_test_extended)\n",
    "r2_extended = r2_score(y_test, y_pred_extended)\n",
    "mse_extended = mean_squared_error(y_test, y_pred_extended)\n",
    "\n",
    "print(f\"R² Score: {r2_extended:.4f}\")\n",
    "print(f\"MSE: {mse_extended:.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse_extended):.4f}\")\n",
    "\n",
    "# Show most important features (largest absolute coefficients)\n",
    "coef_importance = np.abs(extended_poly_reg.coef_)\n",
    "top_indices = np.argsort(coef_importance)[::-1][:10]  # Top 10 features\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(f\"Kbearing_c = {extended_poly_reg.intercept_:.6f}\")\n",
    "for idx in top_indices:\n",
    "    if coef_importance[idx] > 1e-10:  # Only show significant coefficients\n",
    "        print(f\"           + {extended_poly_reg.coef_[idx]:>12.6f} * {feature_names_extended[idx]}\")\n",
    "\n",
    "print(f\"\\nImprovement over basic polynomial: {r2_extended - r2_poly:.4f} R² points\")\n",
    "\n",
    "# Store results for comparison\n",
    "models_extended = {\n",
    "    'Linear': (r2_mlr, mse_mlr, y_pred_mlr),\n",
    "    'Polynomial': (r2_poly, mse_poly, y_pred_poly),\n",
    "    'Extended Polynomial': (r2_extended, mse_extended, y_pred_extended)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Power Law Model (Log-Linear Transformation)\n",
    "print(\"\\n=== 3. Power Law Model ===\")\n",
    "\n",
    "# Check for positive values (required for log transformation)\n",
    "if np.all(X > 0) and np.all(y > 0):\n",
    "    # Log transformation for power law: log(y) = log(A) + α₁log(x₁) + α₂log(x₂) + ...\n",
    "    X_log = np.log(X_train)\n",
    "    y_log = np.log(y_train)\n",
    "    \n",
    "    # Fit log-linear model\n",
    "    power_reg = LinearRegression()\n",
    "    power_reg.fit(X_log, y_log)\n",
    "    \n",
    "    # Predictions (transform back from log space)\n",
    "    X_test_log = np.log(X_test)\n",
    "    y_pred_log = power_reg.predict(X_test_log)\n",
    "    y_pred_power = np.exp(y_pred_log)\n",
    "    \n",
    "    r2_power = r2_score(y_test, y_pred_power)\n",
    "    mse_power = mean_squared_error(y_test, y_pred_power)\n",
    "    \n",
    "    print(f\"R² Score: {r2_power:.4f}\")\n",
    "    print(f\"MSE: {mse_power:.4f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mse_power):.4f}\")\n",
    "    \n",
    "    # Display power law equation\n",
    "    A = np.exp(power_reg.intercept_)\n",
    "    print(f\"\\nPower Law Model:\")\n",
    "    print(f\"Kbearing_c = {A:.4f}\", end=\"\")\n",
    "    for i, (name, alpha) in enumerate(zip(feature_names, power_reg.coef_)):\n",
    "        print(f\" * {name}^{alpha:.4f}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"\\nImprovement over linear: {r2_power - r2_mlr:.4f} R² points\")\n",
    "else:\n",
    "    print(\"Cannot apply power law model: data contains non-positive values\")\n",
    "    y_pred_power = None\n",
    "    r2_power = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Regularized Polynomial Regression (Feature Selection)\n",
    "print(\"\\n=== 4. Regularized Polynomial Regression ===\")\n",
    "\n",
    "# Use Ridge regression to handle overfitting in polynomial model\n",
    "ridge_poly = Ridge(alpha=1.0)\n",
    "ridge_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "y_pred_ridge = ridge_poly.predict(X_test_poly)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"Ridge Regression (α=1.0):\")\n",
    "print(f\"R² Score: {r2_ridge:.4f}\")\n",
    "print(f\"MSE: {mse_ridge:.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse_ridge):.4f}\")\n",
    "\n",
    "# Use Lasso for feature selection\n",
    "lasso_poly = Lasso(alpha=0.1)\n",
    "lasso_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "y_pred_lasso = lasso_poly.predict(X_test_poly)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "\n",
    "print(f\"\\nLasso Regression (α=0.1):\")\n",
    "print(f\"R² Score: {r2_lasso:.4f}\")\n",
    "print(f\"MSE: {mse_lasso:.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse_lasso):.4f}\")\n",
    "\n",
    "# Show selected features (non-zero coefficients)\n",
    "selected_features = feature_names_poly[lasso_poly.coef_ != 0]\n",
    "selected_coefs = lasso_poly.coef_[lasso_poly.coef_ != 0]\n",
    "\n",
    "print(f\"\\nLasso Selected Features ({len(selected_features)} out of {len(feature_names_poly)}):\")\n",
    "print(f\"Kbearing_c = {lasso_poly.intercept_:.4f}\")\n",
    "for name, coef in zip(selected_features, selected_coefs):\n",
    "    print(f\"           + {coef:.6f} * {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Comparison and Visualization\n",
    "print(\"\\n=== Model Comparison Summary ===\")\n",
    "\n",
    "models = {\n",
    "    'Linear': (r2_mlr, mse_mlr, y_pred_mlr),\n",
    "    'Polynomial': (r2_poly, mse_poly, y_pred_poly),\n",
    "    'Ridge': (r2_ridge, mse_ridge, y_pred_ridge),\n",
    "    'Lasso': (r2_lasso, mse_lasso, y_pred_lasso)\n",
    "}\n",
    "\n",
    "if r2_power is not None:\n",
    "    models['Power Law'] = (r2_power, mse_power, y_pred_power)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(models.keys()),\n",
    "    'R² Score': [models[name][0] for name in models.keys()],\n",
    "    'MSE': [models[name][1] for name in models.keys()],\n",
    "    'RMSE': [np.sqrt(models[name][1]) for name in models.keys()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('R² Score', ascending=False)\n",
    "print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_r2 = comparison_df.iloc[0]['R² Score']\n",
    "print(f\"\\nBest Model: {best_model_name} (R² = {best_r2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721368ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualization: Predicted vs Actual\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "plot_idx = 0\n",
    "for name, (r2, mse, y_pred) in models.items():\n",
    "    if plot_idx < len(axes):\n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        # Predicted vs Actual scatter plot\n",
    "        ax.scatter(y_test, y_pred, alpha=0.6, s=20)\n",
    "        \n",
    "        # Perfect prediction line (y=x)\n",
    "        min_val = min(y_test.min(), y_pred.min())\n",
    "        max_val = max(y_test.max(), y_pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "        \n",
    "        ax.set_xlabel('Actual Kbearing_c')\n",
    "        ax.set_ylabel('Predicted Kbearing_c')\n",
    "        ax.set_title(f'{name} Model\\\\nR² = {r2:.4f}, RMSE = {np.sqrt(mse):.4f}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "# Remove empty subplots\n",
    "for idx in range(plot_idx, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance plot for the best polynomial model\n",
    "if best_model_name == 'Polynomial':\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    coef_importance = np.abs(poly_reg.coef_)\n",
    "    sorted_idx = np.argsort(coef_importance)[::-1]\n",
    "    \n",
    "    plt.bar(range(len(coef_importance)), coef_importance[sorted_idx])\n",
    "    plt.xticks(range(len(coef_importance)), [feature_names_poly[i] for i in sorted_idx], rotation=45, ha='right')\n",
    "    plt.ylabel('|Coefficient|')\n",
    "    plt.title('Feature Importance (Polynomial Model)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
